---
title: Python 子进程与子进程池的应用
url: /python-subprocess-process-pool/
date: 2021-09-02T20:44:10-05:00
featured: false
draft: false
type: post
toc: false
# menu: main
usePageBundles: true
thumbnail: "../images/logos/http://unmi.cc/wp-content/uploads/2016/06/python-icon-200x200.png"
categories:
  - Python
tags: 
  - multithread
  - process
comment: true
codeMaxLines: 50
# additional
wpPostId: 11300 
wpStatus: publish
views: 934
lastmod: 2025-05-30T14:24:06-05:00
---

去年记录过一篇如何使用 Python 的线程，线程池的日志 <a href="https://yanbin.blog/python-programming-with-threads/">Python 多线程编程</a>, 需用到 threading.Thread, concurrent.futures.ThreadPoolExecutor。本文可以当作是上一文 Python 多线程编程的姊妹篇。<br/><br/>
Python 的多线程受到 GIL(Global Interpreter Lock) 的限制，GIL 是一把加到了 Python 的解释器的锁，使得在任意时刻只允许一个 Python  进程使用 Python 解释器，也就是任意时刻，Python 只有一个线程在运行。<br/><br/>
GIL 严重影响了计算密集型(CPU-bound) 的多线程程序，此时的多线程与单线程性能没什么差异，也发挥不了多核的威力。但对 I/O 密集型(I/O-bound) 影响不大，因为 CPU 多数时候是在等待。<br/><br/>
为了突破 GIL 的 CPU 密集型程序的限制，可以使用非 CPython 解释器，如 Jython, IronPython 或 PyPy, 更为现实的做法就是使用子进程来替代线程去承担较为繁重的计算任务，因为 GIL 是加在进程上的，所以新的进程有独立的 GIL.<!--more--><br/><br/>
<h2>新建子进程来处理任务</h2><br/><br/>
需用到 multiprocessing.Process 类，这个类在 Python 2.6 就开始存在了。一般的编程语言创建一个进程是要去执行一个外部任命，然后获得它的输出，而 Python 可以像创建线程一样创建子进程，且欲执行的任务直接由 Python 在其中编写<br/><br/>
<pre class="lang:default decode:true">from datetime import datetime
import time
from multiprocessing import Process
import os<br/><br/>
def job(name):
    for _ in range(3):
        print('[Child {}][{}]'.format(os.getpid(), datetime.now()))
        time.sleep(1)<br/><br/>
    print(f'sub process {os.getpid()} {name} done')<br/><br/>
if __name__ == '__main__':
    p = Process(target=job, args=('bob',))   # 留意如何向子进程传递参数
    p.start()
    print(f'main process {os.getpid()} done') # p.pid 可以得到子进程的 ID
    p.join()  # 如果不 join 的话，主进程一退出，子进程也随即结束</pre>
<br/>
执行后输出<br/><br/>
<blockquote>
main process 67474 done<br />
[Child][2021-09-02 19:32:14.121689]<br />
[Child][2021-09-02 19:32:15.126048]<br />
[Child][2021-09-02 19:32:16.129741]<br />
sub process 67476 bob done
</blockquote>
<br/>
可以看到主进程与子进程分别有自己不同的进程 ID。<br/><br/>
还有就是 <code>if __name__ == '__main__'</code> 变得是必要的语句了，不能把想要立即执行的代码丢在函数外了事，否则就是出现下面的错误信息<br/><br/>
<blockquote>
RuntimeError:<br />
            An attempt has been made to start a new process before the<br />
            current process has finished its bootstrapping phase.
            This probably means that you are not using fork to start your<br />
            child processes and you have forgotten to use the proper idiom<br />
            in the main module:
                  if __name__ == '__main__':<br />
                      freeze_support()<br />
                      ...
            The "freeze_support()" line can be omitted if the program<br />
            is not going to be frozen to produce an executable.
</blockquote>
<br/>
进程之间传递参数除了用 <code>args=('bob',)</code> 的任务参数外，还可用 <code>multiprocessing.Pipe</code> 在进程之间双向通信。也可以通过内存来共享数据，用到 <code>multiprocessing.Value</code> 和 <code>multiprocessing.Array</code>。这一部分的详细内容请参考官方文档，在实际编程中可能会经常用到。<br/><br/>
执行进程任务也可以加锁，用 <code>multiprocessing.Lock</code>, <code>lock.acquire()...try: ... finally: lock.release()</code> 标准模式，因为进程之间需要保护对系统中唯一资源的竞争<br/><br/>
<h3>在 Jupyter 中使用 Process 的问题</h3><br/><br/>
如果直接把上面的代码放到 JpyterLab 中去执行，将会看到这样的错误<br/><br/>
<blockquote>
<pre class="lang:default decode:true ">Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "/Users/yanbin/jupyterlab-venv/lib/python3.9/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/Users/yanbin/jupyterlab-venv/lib/python3.9/multiprocessing/spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
AttributeError: Can't get attribute 'job' on &lt;module '__main__' (built-in)&gt;</pre>
</blockquote>
<br/>
这不是 Python 版本的问题，放别的 Python 3.8 下也是这样的错误，原因就是 Jupyter 无法与 multiprocessing 一同工作，pickle 模块在序列化数据向进程发送时出异常。解决办法是要用 <code>multiprocess</code> 替换掉 <code>multiprocessing</code> 模块<br/><br/>
<blockquote>
pip install multiprocess<br />
from multiprocess import Process
</blockquote>
<br/>
然后在 JupyterLab 中执行替换成 multiprocess 的版本，输出略有不同<br/><br/>
<blockquote>
[Child][2021-09-02 19:41:55.326549]<br />
main process 62917 done<br />
[Child][2021-09-02 19:41:56.335774]<br />
[Child][2021-09-02 19:41:57.342169]<br />
sub process 68144 bob done
</blockquote>
<br/>
与在 Python 终端执行的一个区别是，子进程总有一行在 <code>main process ...</code> 之前输出，这没什么要紧的。<br/><br/>
<h2>使用进程池</h2><br/><br/>
有线程池，相应的也有进程池，参照一个官方文档中的简单例子<br/><br/>
<pre class="lang:default decode:true">from multiprocessing import Pool
import os<br/><br/>
def f(x):
    print(f'subprocess id: {os.getpid()}')
    return x*x<br/><br/>
if __name__ == '__main__':
    with Pool(5) as p:
        print(p.map(f, [1, 2, 3])) </pre>
<br/>
输出为<br/><br/>
<blockquote>
subprocess id: 69348<br />
subprocess id: 69350<br />
subprocess id: 69347<br />
[1, 4, 9]
</blockquote>
<br/>
这是用到了 Context 来管理进程池，如果逐步操作就是<br/><br/>
<pre class="lang:default decode:true">pool = Pool(5)
[pool.apply_async(f, args=(i, )) for i in (1, 2, 3)]
pool.close()
pool.join() </pre>
<br/>
<h2>进程池执行器</h2><br/><br/>
这个翻译有点别扭，直接叫  ProcessPoolExecutor 习惯些。ProcessPoolExecutor 在 concurrent.futures 模块中，它是 Python 3.2 加入进来的。至于用法呢，只要想像它是 <code>ThreadPoolExecutor</code> 的进程版本就差不多了。它提供的方法用<br/><br/>
<ol>
    <li>submit(fn, /, *args, **kwargs): 向进程池提交任务</li>
    <li>map(func, *iterables, timeout=None, chunksize=1): 批量提交任务的快捷写法</li>
    <li>shutdown(wait=True, *, cancel_futures=False): 关闭进程池</li>
</ol>
<br/>
首先仍然用一个使用了 <code>with</code> 关键字的写法<br/><br/>
<pre class="lang:default decode:true">from concurrent.futures import ProcessPoolExecutor
import os<br/><br/>
def f(x):
    return f'{os.getpid()}: {x*x}'<br/><br/>
if __name__ == '__main__':
    with ProcessPoolExecutor(max_workers=5) as executor:
        results = executor.map(f, [1, 3, 4])
        for i in results:
            print(i)</pre>
<br/>
输出如下：<br/><br/>
<blockquote>
72088: 1<br />
72090: 9<br />
72089: 16
</blockquote>
<br/>
也可以用 submit() 函数来提交任务，得到的是一个   Future。关于 Future, 以及类似的  submit(), executor.map() 函数在 <a href="https://yanbin.blog/python-programming-with-threads/">Python 多线程编程</a> 有所覆盖。<br/><br/>
另外，在构建 <code>ProcessPoolExecutor</code> 时如果不指  <code>max_workers</code> 参数将会取系统 CPU 的内核数(multiprocessing.cpu_count())。<br/><br/>
如果不对 ProcessPoolExecutor 使用 with 语句，则需要去用 submit() 提交的任务进行 wait，参照<br/><br/>
<pre class="lang:default decode:true ">from concurrent.futures import wait<br/><br/>
tasks = [executor.submit(fn, i) for i in range(5)]
wait(tasks)
</pre>
<br/>
&nbsp;<br/><br/>
链接：<br/><br/>
<ol>
    <li><a href="https://zhuanlan.zhihu.com/p/363040263">一文详解 Python GIL 设计</a></li>
    <li><a href="https://docs.python.org/3.9/library/multiprocessing.html">multiprocessing -- Process-based parallelism</a></li>
    <li><a href="https://www.cnblogs.com/henrytee/p/13526612.html">Python多进程解决方案multiprocessing ProcessPoolExecutor</a></li>
    <li><a href="https://docs.python.org/3/library/concurrent.futures.html">cocurrent.futures -- Launching parallel tasks</a></li>
</ol>
