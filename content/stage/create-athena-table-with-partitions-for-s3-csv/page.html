---
title: 为 S3 中的 CSV 文件创建带 Partition 的 Athena 表
url: /create-athena-table-with-partitions-for-s3-csv/
date: 2022-09-19T14:13:50-05:00
featured: false
draft: true
toc: false
# menu: main
usePageBundles: true
thumbnail: "../images/logos/http://unmi.cc/wp-content/uploads/2017/03/aws-logo.png"
categories:
  - AWS
tags: 
  - Athena
comment: true
codeMaxLines: 50
# additional
wpPostId: 12733 
wpStatus: publish
views: 780
lastmod: 2022-09-19T16:53:11-05:00
---

CSV 文件是纯文本的，对人阅读和编辑来说是最友好的描述表格数据的格式。虽然当前处理大数据时会用到 JSON, avro, parquet 等数据格式，但是在处理平面数据时 CSV 仍然被广泛使用。<br/><br/>
S3 Select 能支持 CSV, JSON 和 parquet 格式数据的直接查询。在 AWS s3 控制台选择一个 CSV 文件，从右上的 <code>Object actions</code> 下拉选项上选择 <code>Query with S3 Select</code> 就能直接查询该文件的内容，而无须下载后打开文件。<br/><br/>
如 S3 Select 查询语句<br/><br/>
<blockquote>
SELECT * from s3object WHERE Name='Tom' LIMIT 5
</blockquote>
<br/>
如果 CSV 带 Header 的话，请勾选上 <code>Exclude the first line of CSV data</code>。当然 S3 Select 查看任意的文本文件也行，只是把它当成一个不规则的 CSV 文件来对待。<br/><br/>
S3 Select 只能针对单个 S3 文件查询，如果要对一组 CSV 文件同时进行查询的话就要用到 Athena。把相同 Schema 的一系列 CSV 文件放到 S3 的某一个目录中，我们可为它们创建一个  Athena 表，然后查询该 Athena 表就会从对应 S3 目录中扫描所有的 CSV 文件。<!--more--><br/><br/>
现在我们来创建两个 CSV 文件<br/><br/>
1.csv<br/><br/>
<blockquote>
Id,Name,Gender,Email,DOB<br />
1,Tom,M,tom@example.com,2000-11-28<br />
2,Jerry,M,jerry@example.com,2001-02-03
</blockquote>
<br/>
2.csv<br/><br/>
<blockquote>
Id,Name,Gender,Email,DOB<br />
3,Anna,F,anna@example.com,2000-05-23<br />
4,Mia,F,mia@example.com,2000-08-07
</blockquote>
<br/>
把它们上传上 s3 的某个目录中<br/><br/>
<blockquote>
aws s3 cp 1.csv s3://yanbin-test-csv/students/<br />
aws s3 cp 2.csv s3://yanbin-test-csv/students/
</blockquote>
<br/>
在 s3://yanbin-test-csv/students/ 目录中就有了 1.csv  和 2.csv 两个文件<br/><br/>
CSV 文件准备好后，我们进入到 Amazon Athena 的查询控制台 <a href="https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#/query-editor">https://us-east-1.console.aws.amazon.com/athena/home?region=us-east-1#/query-editor</a>, 创建 students 表的语句是<br/><br/>
<pre class="lang:default decode:true">CREATE EXTERNAL TABLE IF NOT EXISTS `default.students` (
    `Id` integer,
    `Name` string,
    `Gender` string,
    `Email` string,
    `DOB` date
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION 's3://yanbin-test-csv/students/'
TBLPROPERTIES (
  'skip.header.line.count'='1'
)</pre>
<br/>
创建 Athena 表的语句请参考 <a href="https://docs.aws.amazon.com/athena/latest/ug/create-table.html">Athena CREATE TABLE</a><br/><br/>
<code>skip.header.line.count</code> 跳过第一行的 header<br/><br/>
执行后会在 default 数据库中创建一个 students 表，完后可以执行查询<br/><br/>
<blockquote>
select * from default.students
</blockquote>
<br/>
得到结果<br/><br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-1.png"><img class="aligncenter wp-image-12734" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-1-800x771.png" alt="" width="633" height="610" /></a><br/><br/>
一个基本的为 S3 目录中 CSV 文件创建 Athena 表并查询的功能实现了，现在再往 s3://yanbin-test-csv/students 目录上传一个新的 CSV 文件后，用 select * from default.students 可以查询出新增的记录。<br/><br/>
注意当前的 Athena 执行创建表的语句时错误提示信息还比较弱智，可能任意的语法错误都粗暴的提示为如下错误<br/><br/>
<blockquote>
line 1:8: mismatched input 'EXTERNAL'. Expecting: 'OR', 'SCHEMA', 'TABLE', 'VIEW'
</blockquote>
<br/>
其实产生上面错误的语句是字段类型后多了一个逗号<br/><br/>
<pre class="lang:default decode:true ">CREATE EXTERNAL TABLE IF NOT EXISTS `default.students` ( `Id` integer,)</pre>
<br/>
所以千万不要被它的错误信息误导，并不是 <code>EXTERNAL</code> 的问题，而要认真检查整个语句的语法。<br/><br/>
我们尽管在创建 Athena 表时为字段两边加了斜撇号，但无法保留大小写形式，Athena 总是转换为小写字母形式，因此前面创建表的语句中可去掉字段两边的斜撇号，即使字段以数字开头也可不用斜撇号 -- 数字开头的字段名在查询时需用双引号括起来。<br/><br/>
<code>ROW FORMAT DELIMITED</code> 用于不需要双引号括住值的情况，在 CSV 中值没有逗号用双引号也是合法的，如 <code>"Tom"</code>. <code>ROW FORMAT DELIMITED</code> 也能指分隔换行等属性<br/><br/>
<pre class="lang:default decode:true ">ROW FORMAT DELIMITED
FILEDS TERMINATED BY '\t'
ESACAPED BY '\\'
LINES TERMINATED By '\n'</pre>
<br/>
以上值是默认的。<br/><br/>
<code>ROW FORMAT DELIMITED</code> 默认使用的 SERDE 是 <code>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</code>, 接下来我们要用到另一种 SERDE -- <code>org.apache.hadoop.hive.serde2.OpenCSVSerde</code>。<br/><br/>
还有一个问题，前面创建 Athena 表时只简单认为字段是用逗号分割的，还有稍微复杂一点的 CVS 格式，如字段值含逗号的就需要用双引号括住字段值，再字段值内容中的双引号需要进一步转义<br/><br/>
比如我们编辑一个 3.csv 文件，内容如下<br/><br/>
<blockquote>
Id,Name,Gender,Email,DOB<br />
5,"Tiger,Scott",M,scott@example.com,2001-03-03
</blockquote>
<br/>
然后上传到 s3://yanbin-test-csv/students 目录中，再查询<br/><br/>
<blockquote>
select * from default.students where id=5
</blockquote>
<br/>
得到结果是<br/><br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-3.png"><img class="aligncenter wp-image-12736" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-3-800x100.png" alt="" width="688" height="86" /></a><br/><br/>
显然这不是我们想要的结果，要能识别字段值中的逗号要用到 <code>ROW FORMAT SERDE ...</code> 来指定  CSV 的序列化类<br/><br/>
<pre class="lang:default decode:true">CREATE EXTERNAL TABLE IF NOT EXISTS default.students (
    Id integer,
    Name string,
    Gender string,
    Email string,
    DOB string
)<br/><br/>
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'escapeChar'='\\', 
  'quoteChar'='\"', 
  'separatorChar' = ',',
  'skip.header.line.count'='1'
)
LOCATION 's3://yanbin-test-csv/students/'</pre>
<br/>
再来查询含用 <code>Tiger,Scott</code> 内容的记录，得到的就是<br/><br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-3.png"><img class="aligncenter size-large wp-image-12736" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-3-800x100.png" alt="" width="800" height="100" /></a><br/><br/>
其实 <code>org.apache.hadoop.hive.serde2.OpenCSVSerde</code> 的 <code>escapeChar</code>, <code>quoteChar</code> 和 <code>separatorChar</code> 就是上面的默认值，因此在 <code>WITH SERDEPROPERTIES</code> 中可以省略这三项，简单书写为<br/><br/>
<pre class="lang:default decode:true">CREATE EXTERNAL TABLE IF NOT EXISTS default.students (
    Id integer,
    Name string,
    Gender string,
    Email string,
    DOB string
)<br/><br/>
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'skip.header.line.count'='1'
)
LOCATION 's3://yanbin-test-csv/students/'</pre>
<br/>
参考 <a href="https://docs.aws.amazon.com/athena/latest/ug/csv-serde.html">OpenCSVSerde for processing CSV</a><br/><br/>
OpenCSVSerde 不直接支持字面意义 "YYYY-MM-DD" 格式的 date 类型，所以上面把 DOB 字段类型改成了 string, 它所支持的 date 类型的值是从 1970-01-01 后的天数，如 18276 表示为 2020-01-15，字段值为整数可指定为 date 类型。<br/><br/>
<h3>创建支持 Partition 的 Athena 表</h3><br/><br/>
前面我们把所有的 CSV 文件扔到一个 S3 目录中，所以没有 Partition 支持, 为提升查询效率我们有必要对 CSV 文件分目录，然后映射到 Athena 表的分区中去，查询时就能限定分区来查询，大大缩小扫描文件的数目<br/><br/>
S3 目录到  Athena 表的 Partition 映射约定是<br/><br/>
<blockquote>
s3://yanbin-test-csv/students/year=2000/gender=M/1.csv
</blockquote>
<br/>
映射到 Athena 表的 Partition 名分别为 year=2000, gender=M<br/><br/>
当然这不是自动的，我们需要在创建 Athena 表时指定 Partition 参数<br/><br/>
现在我们把 s3://yanbin-test-csv/students/ 目录清空，根据 year 和  gender 重新组织 CSV 文件，生成以下三个文件<br/><br/>
<blockquote>
$ cat 2000-M.csv<br />
Id,Name,Gender,Email,DOB<br />
1,Tom,M,tom@example.com,2000-11-28<br />
$ cat 2000-F.csv<br />
Id,Name,Gender,Email,DOB<br />
3,Anna,F,anna@example.com,2000-05-23<br />
4,Mia,F,mia@example.com,2000-08-07<br />
$ cat 2001-M.csv<br />
Id,Name,Gender,Email,DOB<br />
2,Jerry,M,jerry@example.com,2001-02-03<br />
5,"Tiger,Scott",M,scott@example.com,2001-03-03
</blockquote>
<br/>
首先上传一个文件到 S3 中去<br/><br/>
<blockquote>
aws s3 cp 2000-M.csv s3://yanbin-test-csv/students/year=2000/gender=M/2000-M.csv
</blockquote>
<br/>
然后创建带 Partition 的 students 表<br/><br/>
<pre class="lang:default decode:true">CREATE EXTERNAL TABLE IF NOT EXISTS default.students (
    Id integer,
    Name string,
    Email string,
    DOB string
)
PARTITIONED BY (year integer, gender string)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'skip.header.line.count'='1'
)
LOCATION 's3://yanbin-test-csv/students/'</pre>
<br/>
由于 <code>PARTITIONED BY</code> 中的字段同时为表的字段，所以必须从表字段中移除 <code>Gender</code> 字段。这时候 Athena 显示 students 为一个 Partitioned 的表<br/><br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-4.png"><img class="aligncenter wp-image-12738" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-4.png" alt="" width="322" height="227" /></a><br/><br/>
我们试着来查询一下<br/><br/>
<blockquote>
select * from default.students<br />
-- 或者用 select * from default.students where year=2000
</blockquote>
<br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-5.png"><img class="aligncenter wp-image-12739" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-5-800x338.png" alt="" width="730" height="308" /></a><br/><br/>
真的没有看错，没有记录，不是明明有一个文件在 s3://yanbin-test-csv/students/year=2000/gender=M/2000-M.csv 吗? <br/><br/>
原因是我们需要创建表时指定了分区为 year 和 gender, 但 Athena 尚未从对应的分区目录中(如 year=2000/gender=M) 中加载分区, 还需要后续一步操作。<br/><br/>
需要执行<br/><br/>
<blockquote>
MSCK REPAIR TABLE <code>default.students</code>
</blockquote>
<br/>
AWS 控制台显示<br/><br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-6.png"><img class="aligncenter wp-image-12740" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-6-800x97.png" alt="" width="742" height="90" /></a><br/><br/>
再用前面的查询语句就能取到数据了<br/><br/>
关于 MSCK REPAIR TABLES 的用法请参数官方文档 <a href="https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html">https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html</a>. MSCK 是 Hive's <strong>M</strong>eta<strong>S</strong>tore consistency check，像 FSCK(file system consistency check) 命名一样。<br/><br/>
后续我们往已知的分区目录 s3://yanbin-test-csv/students/year=2000/gender=M/ 中添加文件就能立即查询到。但是有新的分区目录的话，每次都需要执行语句<br/><br/>
<blockquote>
MSCK REPAIR TABLE default.students
</blockquote>
<br/>
让 Athena 去 s3://yanbin-test-csv/students/ 目录下扫描新的分区，加入到 metastore 信息库中去。<br/><br/>
比如再上传一个文件<br/><br/>
<blockquote>
$ aws s3 cp 2000-F.csv s3://yanbin-test-csv/students/year=2000/gender=F/2000-F.csv
</blockquote>
<br/>
AWS Athena 控制台再执行<br/><br/>
<blockquote>
MSCK REPAIR TABLE default.students
</blockquote>
<br/>
又有 <code>Partition</code> 被添加<br/><br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-7.png"><img class="aligncenter wp-image-12741" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-7-800x97.png" alt="" width="745" height="90" /></a><br/><br/>
这里要留意一个问题，由于创建 Athena 表时字段自动转换为小写，<code>PARTITIONED BY (year integer)</code> 和 <code>PARTITIONED by (Year integer</code> 是一样的，分区字段都是 <code>year</code>，然后 S3 的 Key 是区分大小写的，假如在创建表时恰好写成 <code>PARTITIONED by (Year integer)</code>, 接着把 CSV 文件传到 <code>s3://yanbin-test-csv/students/Year=2000</code> 目录中的话，怎么用 <code>MSCK REPAIR TABLE default.students</code> 命令也添加不了 <code>Year=2000</code> 这个分区，也就查询不到其中的数据。<br/><br/>
我们来作一个测试<br/><br/>
上传文件到 <code>/Year=20001</code> 目录中<br/><br/>
<blockquote>
$ aws s3 cp 2001-M.csv s3://yanbin-test-csv/students/Year=2001/gender=M/2000-M.csv
</blockquote>
<br/>
试图去发现该分区<br/><br/>
<blockquote>
MSCK REPAIR TABLE default.students
</blockquote>
<br/>
<a href="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-8.png"><img class="aligncenter wp-image-12742" src="https://yanbin.blog/wp-content/uploads/2022/09/athena-csv-8-800x78.png" alt="" width="744" height="73" /></a><br/><br/>
Athena 能扫描到该新分区，却无法依照 /year=xxxx/ 的规则加载该分区到 metastore 信息库中去，当然也就无法查询到其中的数据内容<br/><br/>
所以记住，在 S3 路径中的分区名称一定要用小写，这样才能被 <code>MSCK REPAIR TABLE</code> 自动扫描并加载到  metastore 信息库中去。S3 路径中分区的值是区分大小写的，如 <code>/year=M</code>, <code>/year=m</code><br/><br/>
不过 S3 路径中分区分称有大写字母时还有手工的补救措施，在 AWS Athena 控制台执行语句<br/><br/>
<pre class="lang:default decode:true">ALTER TABLE default.students
ADD PARTITION (year='2001', gender='M')
LOCATION 's3://yanbin-test-csv/students/Year=2001/gender=M/'</pre>
<br/>
结果只显示<br/><br/>
<blockquote>
Query successful.
</blockquote>
<br/>
如果再执行就会提示错误<br/><br/>
<blockquote>
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Partition already exists.)
</blockquote>
<br/>
现在就能从新的分区 year=2001, gender=M 中查询到新数据了<br/><br/>
<blockquote>
select id,name,email from default.students where year=2001
</blockquote>
<br/>
当然这种 ADD PARTITION 的操作还能从任意的 S3 目录中加载分区，如<br/><br/>
<pre class="lang:default decode:true">ALTER TABLE default.students
ADD PARTITION (year='2002', gender='F')
LOCATION 's3://other-bucket/students/2001/F/'</pre>
<br/>
实际中请尽量遵循 S3 中分区名部分小写的规则，这样能用 <code>MSCK REPAIR TABLE</code> 自动加载到。<br/><br/>
AWS Athena 控制台中的查询语句还能用 <code>aws cli</code> 来执行，比如<br/><br/>
<blockquote>
$ aws athena start-query-execution --query-string "MSCK REPAIR TABLE default.students"<br />
{<br />
    "QueryExecutionId": "20b481e3-77f1-4333-90fc-52b3ac9d5635"<br />
}
</blockquote>
<br/>
可到 AWS Athena 控制台的 <code>Recent queries</code> 页面中找执行结果，或用 aws cli 查询执行是否成功<br/><br/>
<blockquote>
$ aws athena get-query-execution --query-execution-id 20b481e3-77f1-4333-90fc-52b3ac9d5635
</blockquote>
<br/>
有查询结果的语句，用 <code>get-query-execution</code> 命令能看到 <code>ResultConfiguration:OutputLocation</code> 路径，如 s3://athena-queries/ffa4e9b4-ab92-4e2a-96fb-e8c34881848e.csv<br/><br/>
<blockquote>
$ aws s3 cp s3://athena-queries/ffa4e9b4-ab92-4e2a-96fb-e8c34881848e.csv -
</blockquote>
<br/>
即显示查询结果。<br/><br/>
最后提示：不要轻信 Athena 对查询语句的错误提示，请确保语法正确。什么时候用 ``， "", '' 都没有明确的规则<br/><br/>
<blockquote>
create external table `default.fund-similarity` ...<br />
drop table `default.fund-similarity`<br />
select * from "default"."fund-similarity"    --  不能  select * from <code>default.fund-similarity</code>
</blockquote>
<br/>
链接：<br/><br/>
<ol>
    <li><a href="https://aws.amazon.com/premiumsupport/knowledge-center/athena-create-use-partitioned-tables/">How can I create and use partitioned tables in Amazon Athena?</a></li>
    <li><a href="https://docs.aws.amazon.com/athena/latest/ug/lazy-simple-serde.html">LazySimpleSerDe for CSV, TSV, and custom-delimted files</a></li>
</ol>
