---
title: 使用 Java 转换 Apache Avro 为 Parquet 数据格式
url: /convert-apache-avro-to-parquet-format-in-java/
date: 2021-02-23T23:39:06-06:00
featured: false
draft: true
toc: false
# menu: main
usePageBundles: true
thumbnail: "../images/logos/https://yanbin.blog/wp-content/uploads/2021/02/parquet-logo.jpeg"
categories:
  - Spark
tags: 
  - Avro
  - Parquet
comment: true
codeMaxLines: 50
# additional
wpPostId: 10647 
wpStatus: publish
views: 3312
lastmod: 2021-02-25T21:47:32-06:00
---

Avro 和 Parquet  是处理数据时常用的两种编码格式，它们同为 Hadoop 大家庭中的成员。这两种格式都是自我描述的，即在数据文件中带有 Schema。Avro 广泛的应用于数据的序列化，如 Kafka，它是基于行的格式，可被流式处理，而 Parquet 是列式存储格式的，适合于基于列的查询，不能用于流式处理。</p>
<br/>
既然是一个系统中可能同时用到了这两种数据存储格式，那么就可能有它们之间相互转换的需求。本文探索如何从 Avro 转换为 Parquet 格式数据，以 Java 语言为例，所涉及到的话题有<br/><br/>
<ol>
    <li>转换 Avro 数据为 Parquet 文件</li>
    <li>如何支持 Avro 的 LogicalType 类型到 Parquet 的转换, 以 date 类型为例</li>
    <li>实现转换 Avro 数据为 Parquet 字节数组(内存中完成 Avro 到 Parquet 的转换)</li>
</ol>
<br/>
本文例子中所选择 Avro 版本是当前最新的 1.10.1<!--more--><br/><br/>
<h3>创建 Avro Schema 并编译成 Java 对象</h3><br/><br/>
先来创建一个 Avro schema <code>user.avsc</code>, 并编译成 Java 代码。Schema 定义如下<br/><br/>
<pre class="lang:default decode:true">{
  "namespace": "yanbin.blog.data",
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "name", "type": "string"},
    {"name": "age", "type": ["null", "double"], "default": null}
  ]
}</pre>
<br/>
编译成 Java 代码<br/><br/>
本人在 Mac OS X 平台下用 <code>brew install avro-tools</code> 安装的命令来编译<br/><br/>
<blockquote>
$ avro-tools compile -string schema user.avsc ./
</blockquote>
<br/>
或者用下载的  avro-tools jar 包来编译<br/><br/>
<blockquote>
java -jar /path/to/avro-tools-1.10.1.jar -string compile schema user.avsc ./
</blockquote>
<br/>
或者是用配置的 Maven 插件来编译生成 <code>yanbin.blog.data.User</code> 类。<br/><br/>
<h3>创建 Avro 对象并转换成 Parquet 格式</h3><br/><br/>
把在当前目录中生成的 User 类引入到 Java 项目中，本例用 Maven 来管理依赖，在 pom.xml 中引入最基本的依赖<br/><br/>
<pre class="lang:default decode:true">&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;
        &lt;artifactId&gt;avro&lt;/artifactId&gt;
        &lt;version&gt;1.10.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
        &lt;artifactId&gt;parquet-avro&lt;/artifactId&gt;
        &lt;version&gt;1.11.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;
        &lt;version&gt;1.2.1&lt;/version&gt;
        &lt;exclusions&gt; &lt;!-- hadoop-core 可说是引入了一堆的垃圾，排除所有 --&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;*&lt;/groupId&gt;
                &lt;artifactId&gt;*&lt;/artifactId&gt;
            &lt;/exclusion&gt;
        &lt;/exclusions&gt;
    &lt;/dependency&gt;
    &lt;!-- 补充 hadoop-core 排除的但需要用到的两个包 --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;commons-logging&lt;/groupId&gt;
        &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;
        &lt;version&gt;1.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;commons-configuration&lt;/groupId&gt;
        &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;
        &lt;version&gt;1.6&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</pre>
<br/>
如此，整个项目的依赖就比较干净，用 <code>mvn dependency:tree</code> 命令显示如下：<br/><br/>
<pre class="lang:default decode:true ">[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ test-parquet ---
[INFO] blog.yanbin:test-parquet:jar:1.0-SNAPSHOT
[INFO] +- org.apache.avro:avro:jar:1.10.1:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-core:jar:2.11.3:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.11.3:compile
[INFO] |  |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.11.3:compile
[INFO] |  +- org.apache.commons:commons-compress:jar:1.20:compile
[INFO] |  \- org.slf4j:slf4j-api:jar:1.7.30:compile
[INFO] +- org.apache.parquet:parquet-avro:jar:1.11.1:compile
[INFO] |  +- org.apache.parquet:parquet-column:jar:1.11.1:compile
[INFO] |  |  +- org.apache.parquet:parquet-common:jar:1.11.1:compile
[INFO] |  |  |  \- org.apache.yetus:audience-annotations:jar:0.11.0:compile
[INFO] |  |  \- org.apache.parquet:parquet-encoding:jar:1.11.1:compile
[INFO] |  +- org.apache.parquet:parquet-hadoop:jar:1.11.1:compile
[INFO] |  |  +- org.apache.parquet:parquet-jackson:jar:1.11.1:compile
[INFO] |  |  +- org.xerial.snappy:snappy-java:jar:1.1.7.3:compile
[INFO] |  |  \- commons-pool:commons-pool:jar:1.6:compile
[INFO] |  \- org.apache.parquet:parquet-format-structures:jar:1.11.1:compile
[INFO] |     \- javax.annotation:javax.annotation-api:jar:1.3.2:compile
[INFO] +- org.apache.hadoop:hadoop-core:jar:1.2.1:compile
[INFO] +- commons-logging:commons-logging:jar:1.2:compile
[INFO] \- commons-configuration:commons-configuration:jar:1.6:compile
[INFO]    +- commons-collections:commons-collections:jar:3.2.1:compile
[INFO]    +- commons-lang:commons-lang:jar:2.4:compile
[INFO]    +- commons-digester:commons-digester:jar:1.8:compile
[INFO]    |  \- commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO]    \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------</pre>
<br/>
Java 代码如下<br/><br/>
<pre class="lang:default decode:true">package yanbin.blog;<br/><br/>
import org.apache.avro.Schema;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetFileWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import yanbin.blog.data.User;<br/><br/>
import java.io.IOException;
import java.io.UncheckedIOException;
import java.util.Arrays;
import java.util.List;<br/><br/>
public class TestParquet {<br/><br/>
    public static void main(String[] args) {
        List&lt;User&gt; users = Arrays.asList(
                User.newBuilder().setId(1).setName("Scott").build(),
                User.newBuilder().setId(2).setName("Tiger").setAge(20.5).build());
        writeToParquet(users);
    }<br/><br/>
    public static &lt;T extends SpecificRecordBase&gt; void writeToParquet(List&lt;T&gt; avroObjects) {
        Schema avroSchema = avroObjects.get(0).getSchema();
        String parquetFile = "./users.parquet";
        Path path = new Path(parquetFile);
        try (ParquetWriter&lt;Object&gt; writer = AvroParquetWriter.builder(path)
                .withSchema(avroSchema)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)
                .build()) {
            avroObjects.forEach(r -&gt; {
                try {
                    writer.write(r);
                } catch (IOException ex) {
                    throw new UncheckedIOException(ex);
                }
            });
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}</pre>
<br/>
执行后在当前目录下生成了文件 <code>users.parquet</code>, 接下来看看这个文件中都有什么。<br/><br/>
类似于安装 <code>avro-tools</code>, 我们可以本 Mac OS X下用 <code>brew install parquet-tools</code> 安装 parquet 的命令，其他平台使用各自的包管理工具来安装 parquet-tools 命令。<br/><br/>
查看 <code>users.parquet</code> 中的数据<br/><br/>
<blockquote>
$ parquet-tools cat --json users.parquet<br />
{"id":1,"name":"Scott"}<br />
{"id":2,"name":"Tiger","age":20.5}
</blockquote>
<br/>
查看 <code>users.parquet</code> 数据的 Schema<br/><br/>
<blockquote>
$ parquet-tools schema users.parquet<br />
message yanbin.blog.data.User {<br />
  required int32 id;<br />
  required binary name (STRING);<br />
  optional double age;<br />
}
</blockquote>
<br/>
从中可以看到 Avro Schema 到 Parquet Schema 的映射，"int" 为 "int32", "string" 为 binary, 有 default 的字段在 Parquet 中会加上个 optional.<br/><br/>
<h3>LogicalType 的转换</h3><br/><br/>
Avro 和 Parquet 的数据类型都有自己的 LogicType 概念，下面给 <code>user.asvc</code> 加上一个 <code>date</code> LogicalType 的 birthday 字段，整个 <code>user.avsc</code> 内容如下<br/><br/>
<pre class="lang:default mark:9 decode:true">{
  "namespace": "yanbin.blog.data",
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "name", "type": "string"},
    {"name": "age", "type": ["null", "double"], "default": null},
    {"name": "birthday", "type": ["null", {"type": "int", "logicalType": "date"}], "default": null}
  ]
}</pre>
<br/>
再次用 <code>avro-tools</code> 命令编译为 <code>User.java</code> 文件，在 User 类中 <code>birthday</code> 的类型是 <code>java.time.LocalDate</code>。再试图用之前的代码来转换带有 <code>birthday</code> 值的 Avro 对象，把前面 <code>TestParquet</code> 的 <code>main</code> 方法修改如下：<br/><br/>
<pre class="lang:default decode:true">    public static void main(String[] args) {
        List&lt;User&gt; users = Arrays.asList(
                User.newBuilder().setId(1).setName("Scott").setBirthday(LocalDate.now()).build(),
                User.newBuilder().setId(2).setName("Tiger").setAge(20.5).build());
        writeToParquet(users);
    }</pre>
<br/>
执行后提示错误<br/><br/>
<blockquote>
Exception in thread "main" java.lang.ClassCastException: java.time.LocalDate cannot be cast to java.lang.Number<br />
    at org.apache.parquet.avro.AvroWriteSupport.writeValueWithoutConversion(AvroWriteSupport.java:323)<br />
    at org.apache.parquet.avro.AvroWriteSupport.writeValue(AvroWriteSupport.java:275)<br />
    at org.apache.parquet.avro.AvroWriteSupport.writeRecordFields(AvroWriteSupport.java:191)<br />
    at org.apache.parquet.avro.AvroWriteSupport.write(AvroWriteSupport.java:165)<br />
    at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)<br />
    at org.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:301)
</blockquote>
<br/>
为了处理这个错误，我还跟到了源代码中，停在 <code>org.apache.avro.generic.GenericData</code> 类中方法<br/><br/>
<blockquote>
public &lt;T&gt; Conversion&lt;T&gt; getConversionByClass(Class&lt;T&gt; datumClass, LogicalType logicalType)
</blockquote>
<br/>
的代码行<br/><br/>
<blockquote>
Map&lt;String, Conversion&lt;?&gt;&gt; conversions = (Map)this.conversionsByClass.get(datumClass);
</blockquote>
<br/>
处给 <code>conversionsByClass</code> 添加个值也能解决这个问题，调试时执行代码<br/><br/>
<blockquote>
this.conversionsByClass.put(LocalDate.class, ImmutableMap.of("date", new TimeConversions.DateConversion()))
</blockquote>
<br/>
后来仔细检查 <code>AvroParquetWriter</code>  类可以调用 <code>withDataModel(GenericData)</code>  来添加 Conversion, 于是 <code>writeToParquet()</code> 方法的实现如下<br/><br/>
<pre class="lang:default mark:5-6,8 decode:true">    public static &lt;T extends SpecificRecordBase&gt; void writeToParquet(List&lt;T&gt; avroObjects) {
        Schema avroSchema = avroObjects.get(0).getSchema();
        String parquetFile = "./users.parquet";
        Path path = new Path(parquetFile);
        GenericData genericData = GenericData.get();
        genericData.addLogicalTypeConversion(new TimeConversions.DateConversion());
        try (ParquetWriter&lt;Object&gt; writer = AvroParquetWriter.builder(path)
                .withDataModel(genericData)
                .withSchema(avroSchema)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)
                .build()) {
            avroObjects.forEach(r -&gt; {
                try {
                    writer.write(r);
                } catch (IOException ex) {
                    throw new UncheckedIOException(ex);
                }
            });
        } catch (IOException e) {
            e.printStackTrace();
        }
    }</pre>
<br/>
执行 TestParquet  类产生新的 <code>users.parquet</code>  文件，再次查看它的数据和 Schema<br/><br/>
<blockquote>
$ parquet-tools cat --json users.parquet<br />
{"id":1,"name":"Scott","birthday":18681}<br />
{"id":2,"name":"Tiger","age":20.5}
$ parquet-tools schema users.parquet<br />
message yanbin.blog.data.User {<br />
  required int32 id;<br />
  required binary name (STRING);<br />
  optional double age;<br />
  optional int32 birthday (DATE);<br />
}
</blockquote>
<br/>
其他的 LogicalType 应该也可以采用类似的方式来处理。如果我们查看 Conversion 的实现类有以下 9 个<br/><br/>
<a href="https://yanbin.blog/wp-content/uploads/2021/02/avro-parquet-1.png"><img class="aligncenter wp-image-10652" src="https://yanbin.blog/wp-content/uploads/2021/02/avro-parquet-1-800x281.png" alt="" width="586" height="206" /></a>必要是想必可以实现自己的 Conversion 类<br/><br/>
其实 Parquet 也有自己的 LogicalType 定义，如 MAP,DECIMAL, DATE, TIME, TIMESTAMP 等，如何把 Avro 的 date 类型映射为 Parquet 的 DATE 类型也是个问题。<br/><br/>
<h3>如何在内存中完成 Avro 转换为 Parquet </h3><br/><br/>
以上转换 Avro 为 Parquet 需要生成一个文件，是否能在内存中完成 Avro 到 Parquet 格式的转换呢？即要得到 Parquet  内容的字节数组而不借助于磁盘文件。突破口应该是在 `AvroParquetWriter.builder()` 这个方法上，它有两个重载方法，分别是<br/><br/>
<pre class="lang:default decode:true ">  public static &lt;T&gt; Builder&lt;T&gt; builder(Path file) {
    return new Builder&lt;T&gt;(file);
  }<br/><br/>
  public static &lt;T&gt; Builder&lt;T&gt; builder(OutputFile file) {
    return new Builder&lt;T&gt;(file);
  }</pre>
<br/>
Path 是一个 <code>org.apache.hadoop.fs.Path</code> 类, 而非 Java 的 Path，这个估计不行，OutputFile 是一个接口 <code>org.apache.parquet.io.OutputFile</code>, 看起来有戏，它目前只有一个实现类 <code>org.apache.parquet.hadoop.util.HadoopOutputFile</code>, 要实现内存中存储字节类容的 <code>OutputFile</code> 肯定是可行的。<br/><br/>
继续追踪 OutputFile 的 create 方法的返回值为 <code>PositionOutputStream</code>, 它有两个实现 <code>DelegatingPositionOutputStream</code> 和 <code>HadoopPositionOutputStream</code>, 前者是一个适配器. <code>DelegatingPositionOutputStream</code> 的构造函数需要一个  <code>OutputStream</code>，把它换成一个 <code>ByteArrayOutpuStream</code> 就能在内存中处理了。最后只要实现 <code>PositionOutputStream</code>  的 <code>long getPos()</code> 返回 <code>ByteArrayOutputStream</code>  的当前位置就行。<br/><br/>
具体实现需要创建一个  <code>InMemoryOutputFile</code> 和 <code>InMemoryPositionOutputStream</code>  类, 写在一个类文件  <code>InMemoryOutputFile.java</code> 中<br/><br/>
<pre class="lang:default decode:true">package yanbin.blog;<br/><br/>
import org.apache.parquet.io.DelegatingPositionOutputStream;
import org.apache.parquet.io.OutputFile;
import org.apache.parquet.io.PositionOutputStream;<br/><br/>
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.OutputStream;<br/><br/>
public class InMemoryOutputFile implements OutputFile {
    private final ByteArrayOutputStream baos = new ByteArrayOutputStream();<br/><br/>
    @Override
    public PositionOutputStream create(long blockSizeHint) throws IOException { // Mode.CREATE 会调用此方法
        return new InMemoryPositionOutputStream(baos);
    }<br/><br/>
    @Override
    public PositionOutputStream createOrOverwrite(long blockSizeHint) throws IOException {
        return null;
    }<br/><br/>
    @Override
    public boolean supportsBlockSize() {
        return false;
    }<br/><br/>
    @Override
    public long defaultBlockSize() {
        return 0;
    }<br/><br/>
    public byte[] toArray() {
        return baos.toByteArray();
    }<br/><br/>
    private static class InMemoryPositionOutputStream extends DelegatingPositionOutputStream {<br/><br/>
        public InMemoryPositionOutputStream(OutputStream outputStream) {
            super(outputStream);
        }<br/><br/>
        @Override
        public long getPos() throws IOException {
            return ((ByteArrayOutputStream) this.getStream()).size();
        }
    }
}</pre>
<br/>
应用 <code>InMemoryOutputFile</code>  的 <code>writeToParquet()</code> 方法，为验证内存中的内容是否正确，我们把 outputFile.toArray() 输出到 users-memory.parquet 文件<br/><br/>
<pre class="lang:default mark:6,10,23 decode:true">    public static &lt;T extends SpecificRecordBase&gt; void writeToParquet(List&lt;T&gt; avroObjects) throws IOException {
        Schema avroSchema = avroObjects.get(0).getSchema();
        GenericData genericData = GenericData.get();
        genericData.addLogicalTypeConversion(new TimeConversions.DateConversion());
        InMemoryOutputFile outputFile = new InMemoryOutputFile();
        try (ParquetWriter&lt;Object&gt; writer = AvroParquetWriter.builder(outputFile)
                .withDataModel(genericData)
                .withSchema(avroSchema)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .withWriteMode(ParquetFileWriter.Mode.CREATE) // 内存中处理什么 Mode 无所谓
                .build()) {
            avroObjects.forEach(r -&gt; {
                try {
                    writer.write(r);
                } catch (IOException ex) {
                    throw new UncheckedIOException(ex);
                }
            });
        } catch (IOException e) {
            e.printStackTrace();
        }<br/><br/>
        Files.write(Paths.get("./users-memory.parquet"), outputFile.toArray());
    }</pre>
<br/>
执行 <code>TestParquest</code> 后生成  <code>users-memory.parquet</code> 文件，现在是有点激动人心的时刻，验证内存中的内容是否正确<br/><br/>
<blockquote>
$ ls -l *.parquet<br />
-rw-r--r-- 1 yanbin root 1173 Feb 23 23:19 users-memory.parquet<br />
-rwxrwxrwx 1 yanbin root 1173 Feb 23 22:49 users.parquet<br />
$<br />
$ parquet-tools cat --json users-memory.parquet<br />
{"id":1,"name":"Scott","birthday":18681}<br />
{"id":2,"name":"Tiger","age":20.5}<br />
$<br />
$ parquet-tools schema users-memory.parquet<br />
message yanbin.blog.data.User {<br />
  required int32 id;<br />
  required binary name (STRING);<br />
  optional double age;<br />
  optional int32 birthday (DATE);<br />
}
</blockquote>
<br/>
文件大小相同，内容和 Schema 都无误，大功告成，洗洗睡了。<br/><br/>
爬起来再被一句，内存中处理的话小心消耗内存，快速处理小文件不错，避免了磁盘 IO 操作，但大文件就会要命的。
